{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastsparse.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastSparse\n",
    "\n",
    "> Customizable Fastai+PyTorch implementation of sparse model training methods (SET, SNFS, RigL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Warning: this repo is undergoing active development_\n",
    "\n",
    "TODOs:\n",
    " - test dynamic training callback\n",
    " - finish documenting this page\n",
    "  - PyTorch example\n",
    "  - under-the-hood explanation\n",
    "  - fully custom example\n",
    "  - Drop/Redist/Grow criterion\n",
    " - implement distributed training (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install fastsparse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this package, you can train your model using the latest dynamic sparse training techniques. It only takes 4 additional lines of code!\n",
    "\n",
    "```python\n",
    "from fastai.vision.all import *\n",
    "from fastsparse.core import *                            # <-- import this package\n",
    "\n",
    "path = untar_data(URLs.MNIST)\n",
    "dls = ImageDataLoaders.from_folder(path, 'training', 'testing')\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate, pretrained=False)\n",
    "sparse_hooks = sparsify_model(learn.model, sparsity=0.9) # <-- initial sparsity + enforce masks\n",
    "cbs = DynamicSparseTrainingCallback(**RigL_kwargs)       # <-- dynamic mask updates\n",
    "\n",
    "learn.fit_one_cycle(1, cbs=cbs)\n",
    "\n",
    "for h in sparse_hooks: h.remove()                        # <-- stop enforcing masks\n",
    "```\n",
    "\n",
    "Simply omit the `DynamicSparseTrainingCallback` to train a fixed-sparsity model as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch example\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Large Batch Sizes\n",
    "\n",
    "Authors of the Rigged Lottery paper hypothesize that the effectiveness of using the gradient magnitude for determining which connections to grow is partly due to their large batch size (4096 for ImageNet). Those without access to multi-gpu clusters can achieve effective batch sizes of this size by using fastai's `GradientAccumulation` callback, which has been tested to be compatible with this package's `DynamicSparseTrainingCallback`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-The-Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what's going on. \n",
    "\n",
    "When you run `sparsify_model(learn.model, 0.9)`, this adds sparse masks and add pre_forward hooks to enforce masks on weights during forward pass.\n",
    "\n",
    "> By default, a uniform sparsity distribution is used. Change the sparsity distribution to Erdos-Renyi with `sparsify_model(learn.model, 0.9, sparse_init_f=erdos_renyi)`, or pass in your custom function (see [Customization](#Customization)\n",
    "\n",
    "> To avoid adding pre_forward hooks, use `sparsify_model(learn.model, 0.9, enforce_masks=False)`.\n",
    "\n",
    "When you add the `DynamicSparseTrainingCallback` callback, ... TODO complete section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization\n",
    "\n",
    "There are several places to modify the behavior of fastsparse to accomplish custom behaviors. For an example, check out the implementation of RigL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial sparsity distribution:\n",
    "\n",
    "Define your own initial sparsity distribution by setting `sparsify_method` in `sparsify_model` to a custom function. For example, this function (included in library) makes the first layer dense, and all following layers to a fixed sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_layer_dense_uniform(params:list, model_sparsity:float):\n",
    "    sparsities = [1.] + [model_sparsity] * (len(params) - 1)\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Drop Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Redistribute Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Grow Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
