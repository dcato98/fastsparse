{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Core\n",
    "\n",
    "> Core functionality for sparsifying dense modules & models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai.basics import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.test_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Module\n",
    "\n",
    "> For sparsifying a single module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a parameter and buffer in a module follow the naming convention: `{p_name}`, `{p_name}_mask`, respectively, the buffer is assumed to be the mask for the parameter. For example, masked Linear and ConvNd layers will typically have a parameter named `weight`, a buffer named `weight_mask`. Additionally, parameters optionally also contain a sparsity buffer (e.g. for ConvNd, named `weight_sparsity`), which is used by the DynamicSparseTrainingCallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparse_mask(sizes, sparsity):\n",
    "    '''Returns a boolean mask with uniformly distributed zeros. # zeros = `sparsity` * np.prod(`sizes`)'''\n",
    "    n_total = np.prod(sizes)\n",
    "    n_ones = round((1-sparsity) * n_total)\n",
    "    shuffled_ones = torch.randperm(n_total)[:n_ones]\n",
    "    mask = torch.zeros(n_total, dtype=torch.bool)\n",
    "    mask[shuffled_ones] = True\n",
    "    return mask.reshape(*sizes)\n",
    "\n",
    "def sparse_mask_like(param, sparsity): return sparse_mask(param.shape, sparsity).to(param.device)\n",
    "def mask_from_tensor(t): return t.ne(0)\n",
    "def sparsity_from_tensor(t): return 1 - mask_from_tensor(t).sum() / t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sparse_mask((10,5), 0.8)\n",
    "test_eq(10, int(mask.sum()))\n",
    "\n",
    "t = torch.rand(3,10)\n",
    "mask = sparse_mask_like(t, 0.8)\n",
    "t.mul_(mask)\n",
    "test_eq(mask, mask_from_tensor(t))\n",
    "test_close(0.8, sparsity_from_tensor(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maybe_float(num):\n",
    "    try: return float(num)\n",
    "    except: return num\n",
    "    \n",
    "def sparse_params(module):\n",
    "    '''Returns list of all (param, mask, sparsity) tuples in a module.'''\n",
    "    buffer_d = {name:b for name, b in module.named_buffers()}\n",
    "    param_mask_sparsities = [(p, buffer_d[f'{name}_mask'], maybe_float(buffer_d.get(f'{name}_sparsity')))\n",
    "                             for name, p in module.named_parameters() \n",
    "                             if f'{name}_mask' in buffer_d]\n",
    "    return list(set(param_mask_sparsities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = 0.8, nn.Linear(5,10)\n",
    "m.register_buffer('weight_mask', sparse_mask_like(m.weight, s))\n",
    "m.register_buffer('weight_sparsity', tensor(s))\n",
    "m.register_buffer('bias_mask', sparse_mask_like(m.bias, s))\n",
    "param_mask_sparsity = sparse_params(m)\n",
    "test_eq(2, len(param_mask_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def apply_masks(module, *args, inplace=True):\n",
    "    for param, mask, sparsity in sparse_params(module):\n",
    "        if inplace: param.data.mul_(mask)\n",
    "        else:       param.data = param.data.mul(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_masks(m)\n",
    "test_eq(10, m.weight.abs().gt(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_sparseable_module_types = (nn.Linear, \n",
    "                            nn.Conv1d, nn.Conv2d, nn.Conv3d, \n",
    "                            nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,\n",
    "                            nn.MultiheadAttention,\n",
    "                            nn.RNN, nn.RNNCell, nn.GRU, nn.GRUCell, nn.LSTM, nn.LSTMCell)\n",
    "\n",
    "def is_sparseable_module(m, additional_types=[]):\n",
    "    types = set(_sparseable_module_types) | set(additional_types)\n",
    "    return isinstance(m, tuple(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# TODO: flatten_model gets rid of nn.MultiheadAttention which has it's own parameter 'in_proj_weight'\n",
    "#       which means sparsity_model doesn't sparsify this parameter\n",
    "def sparseable_modules(model, additional_types=[]):\n",
    "    filt = partial(is_sparseable_module, additional_types=additional_types)\n",
    "    return L(flatten_model(model)).filter(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3,32,3), nn.ReLU(), \n",
    "        nn.Conv2d(32,128,3), nn.ReLU(), \n",
    "        nn.Conv2d(128,512,3), nn.ReLU(), AdaptiveAvgPool(), Flatten(),\n",
    "        nn.Linear(512, 10))\n",
    "\n",
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "test_eq(4, len(s_mods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mask_from_tensor(t): return t != 0\n",
    "def sparsity_from_tensor(t): return 1 - mask_from_tensor(t).sum() / t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3,10)\n",
    "mask = sparse_mask_like(t, 0.8)\n",
    "t.mul_(mask)\n",
    "test_eq(mask, mask_from_tensor(t))\n",
    "test_close(0.8, sparsity_from_tensor(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Weight Initialization\n",
    "\n",
    "> Sparsifying weights changes the variance, so we need to adjust the usual kaiming normal initialization. \n",
    "\n",
    "Correctly initializing a network can improve the speed and accuracy of training by ensuring good forward signal propagation and backward gradient flow. This is especially important in networks without batch normalization or skip connections. In sparse networks, initalizing the variance to the harmonic mean of fan_in and fan_out can improve training results. For example, using 'fan_in_out' initialization on MNIST+LeNet5 results in similar or improved accuracies compared to nn.Linear's default dense initialization).\n",
    "\n",
    "See [Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win](https://arxiv.org/abs/2010.03533) by Utku Evci et al. for a more in-depth discussion of sparse weight initialization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def init_kaiming_normal_sparse_(t, a=0, mode='fan_in', sparse_mode='fan_in_out', nonlinearity='leaky_relu'):\n",
    "    '''A modified kaiming normal initialization which adjusts for sparsity in weights.'''\n",
    "    # calculate sparse adjustment to standard deviation\n",
    "    #  dense kaiming init = mode / sqrt(dense_fan), e.g. for relu = 2 / sqrt(dense_fan)\n",
    "    #  sparse kaiming init = mode / sqrt(sparse_fan), note: sparse fan is unique to each input/output\n",
    "    #                      = (dense kaiming init) * sqrt(dense_fan / sparse_fan)\n",
    "    mask = mask_from_tensor(t)\n",
    "    mode = mode if mask.sum() == t.numel() else sparse_mode\n",
    "    mode_ix = ['fan_in', 'fan_out', 'fan_in_out'].index(mode)\n",
    "    dim = [1,0,1][mode_ix]\n",
    "    \n",
    "    dense_fan = t.shape[dim] * t[0][0].numel()\n",
    "    \n",
    "    sparse_fan_in = mask.sum(1, keepdim=True)\n",
    "    sparse_fan_out = mask.sum(0, keepdim=True)\n",
    "    # variance of 'fan_in_out' is harmonic mean of 'fan_in' and 'fan_out'\n",
    "    sparse_fan_in_out = (sparse_fan_in + sparse_fan_out) / 2\n",
    "    \n",
    "    sparse_fan = [sparse_fan_in, sparse_fan_out, sparse_fan_in_out][mode_ix]\n",
    "    sparse_fan[sparse_fan==0] = 1 # avoid div by 0, can set to anything since these are masked\n",
    "    \n",
    "    std_adj = torch.sqrt(dense_fan / sparse_fan)\n",
    "    \n",
    "    # initialize as dense, then apply mask and apply sparse adjustment\n",
    "    mode = 'fan_in' if mode == 'fan_in_out' else mode\n",
    "    nn.init.kaiming_normal_(t, a=a, mode=mode, nonlinearity=nonlinearity)\n",
    "    return t.mul_(mask).mul_(std_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense            : tensor(0.6723, grad_fn=<VarBackward0>) tensor(3.1998e-18) tensor(1.)\n",
      "sparse no adj    : tensor(0.0067, grad_fn=<VarBackward0>) tensor(3.2092e-20) tensor(0.0100)\n",
      "sparse fan_in    : tensor(0.6646, grad_fn=<VarBackward0>) tensor(3.1882e-18) tensor(0.9964)\n",
      "sparse fan_out   : tensor(0.0134, grad_fn=<VarBackward0>) tensor(6.4054e-20) tensor(0.0200)\n",
      "sparse fan_in_out: tensor(0.0263, grad_fn=<VarBackward0>) tensor(1.2513e-19) tensor(0.0391)\n"
     ]
    }
   ],
   "source": [
    "def backward_variance(t):\n",
    "    t.grad = None\n",
    "    loss = mse(t, torch.zeros_like(t)).sum()\n",
    "    loss.backward()\n",
    "    var = t.grad.var()\n",
    "    return var\n",
    "\n",
    "# here we compare the variance of a dense matrix multiply and a several sparse matrix multiply ops\n",
    "t1 = torch.rand(10,1000)      # this is our input\n",
    "t2 = torch.rand(50000,1000)   # this is our weight matrix\n",
    "t2.requires_grad_(True)\n",
    "nn.init.kaiming_normal_(t2)\n",
    "var1 = torch.var(t1 @ t2.t()) # dense variance\n",
    "bvar1 = backward_variance(t2)\n",
    "\n",
    "t2.requires_grad_(False)\n",
    "mask = sparse_mask_like(t2, 0.99)\n",
    "t2 = t2.mul_(mask)            # sparsify our weight matrix\n",
    "t2.requires_grad_(True)\n",
    "\n",
    "var2 = torch.var(t1 @ t2.t()) # variance before adjustment\n",
    "bvar2 = backward_variance(t2)\n",
    "\n",
    "init_kaiming_normal_sparse_(t2, sparse_mode='fan_in')\n",
    "var3 = torch.var(t1 @ t2.t()) # variance after fan_in adjustment\n",
    "bvar3 = backward_variance(t2)\n",
    "\n",
    "init_kaiming_normal_sparse_(t2, sparse_mode='fan_out').abs().gt(0).sum()\n",
    "var4 = torch.var(t1 @ t2.t()) # variance after fan_out adjustment\n",
    "bvar4 = backward_variance(t2)\n",
    "\n",
    "init_kaiming_normal_sparse_(t2, sparse_mode='fan_in_out').abs().gt(0).sum()\n",
    "var5 = torch.var(t1 @ t2.t()) # variance after fan_in_out adjustment\n",
    "bvar5 = backward_variance(t2)\n",
    "\n",
    "test_ne(var1, var2)\n",
    "test_close(var1, var3, eps=0.02)\n",
    "\n",
    "print('dense            :', var1, bvar1, bvar1 / bvar1)\n",
    "print('sparse no adj    :', var2, bvar2, bvar2 / bvar1)\n",
    "print('sparse fan_in    :', var3, bvar3, bvar3 / bvar1)\n",
    "print('sparse fan_out   :', var4, bvar4, bvar4 / bvar1)\n",
    "print('sparse fan_in_out:', var5, bvar5, bvar5 / bvar1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Distributions\n",
    "\n",
    "> For determining the layer-wise sparsity of a list of modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "> All layers have a the same percentage of connection removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_sparsity(params, model_sparsity):\n",
    "    return [model_sparsity] * len(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Layer-Dense Uniform Distribution\n",
    "\n",
    "> Uniform sparsity except for the first layer, which is dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def first_layer_dense_uniform(params, model_sparsity):\n",
    "    sparsities = [0.] + [model_sparsity] * (len(params) - 1)\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erdos-Renyi (Kernel) Distribution\n",
    "\n",
    "> For a fixed overall sparsity, the Erdos-Renyi sparsity distribution allocates more connections to smaller layers and fewer to large layers when compared to a uniform sparsity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# modified from https://github.com/google-research/rigl/blob/master/rigl/sparse_utils.py.\n",
    "def erdos_renyi_sparsity(params, model_sparsity, include_kernel=True, erk_power_scale=1.0):\n",
    "    \"\"\"\n",
    "    Returns a list of sparsities in the same order as params. Sparsities satisfy \n",
    "    the Erdos-Renyi(Kernel) distribution, where the model has a total parameter count \n",
    "    as one with uniform sparsities, that is, satisfying the following equation:\n",
    "    $ eps * (p_1 * N_1 + p_2 * N_2) = (1 - model_sparsity) * (N_1 + N_2) $, for some float `eps`.\n",
    "    \n",
    "    Args:\n",
    "    params: list of all sparseable parameters\n",
    "    model_sparsity: target overall sparsity between 0 and 1\n",
    "    include_kernel: if True, kernel dimensions are included in the scaling (e.g. for ConvNd layers)\n",
    "    erk_power_scale: scale < 1 softens the erdos_renyi distribution (i.e. closer to uniform)\n",
    "    \n",
    "    Returns a list of sparsities where values correspond to individual param sparsities.\n",
    "    \"\"\"\n",
    "    # Enforce custom sparsities, then find correct scaling factor, `eps` for remaining params\n",
    "    dense_layers = set()\n",
    "    is_eps_valid = False\n",
    "    while not is_eps_valid:\n",
    "        # Start with all layers and try to find right eps. If any sparsity exceeds 1, \n",
    "        # make that layer dense and repeat with the non-dense layers.\n",
    "        #\n",
    "        # E.g. where N_3, and N_4 are found to be dense:\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) + (N_3 + N_4) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2 + N_3 + N_4)\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2) - model_sparsity * (N_3 + N_4) <--- == rhs\n",
    "        # eps = rhs / (\\sum_i p_i * N_i) <--- == divisor\n",
    "        # eps = rhs / divisor\n",
    "\n",
    "        divisor = 0\n",
    "        rhs = 0\n",
    "        raw_sparsity = {}\n",
    "        for p in params:\n",
    "            n_zeros = int(np.floor(model_sparsity * p.numel()))\n",
    "            if p in dense_layers:\n",
    "                rhs -= n_zeros\n",
    "            else:\n",
    "                n_ones = p.numel() - n_zeros\n",
    "                rhs += n_ones\n",
    "                if include_kernel:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape) / np.prod(p.shape))**erk_power_scale\n",
    "                else:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape[:2]) / np.prod(p.shape[:2]))\n",
    "                divisor += raw_sparsity[p] * p.numel()\n",
    "                \n",
    "        eps = rhs / divisor\n",
    "        \n",
    "        # If eps * raw_sparsity[p] > 1, we add the param to the set of dense_layers\n",
    "        max_sparsity = np.max(list(raw_sparsity.values()))\n",
    "        if eps * max_sparsity > 1:\n",
    "            for p, p_raw_sparsity in raw_sparsity.items():\n",
    "                if p_raw_sparsity == max_sparsity:\n",
    "                    dense_layers.add(p)\n",
    "        else:\n",
    "            is_eps_valid = True\n",
    "\n",
    "    # With the valid eps, we can set sparsities of the remaining layers\n",
    "    sparsities = [0. if p in dense_layers else (1. - eps * raw_sparsity[p]) for p in params]\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_params = L(sparseable_modules(model)).map(lambda m: m.weight)\n",
    "\n",
    "sparsities = erdos_renyi_sparsity(s_params, 0.9)\n",
    "n_nonzeros = sum([(1-s) * p.numel() for p, s in zip(s_params, sparsities)])\n",
    "test_close(n_nonzeros, 0.1 * sum([p.numel() for p in s_params]), eps=len(s_params))\n",
    "# test_eq([0., 0., 0., 0.], sparsities) # TODO: calc sparsities by hand and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Model\n",
    "\n",
    "> For sparsifying an entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparsify_model(model, model_sparsity, sparse_f=uniform_sparsity, \n",
    "                   sparse_init_mode=None, enforce_mask=True):\n",
    "    '''\n",
    "    Adds a sparse mask for each sparseable-module weight in model and applies mask to weights.\n",
    "    \n",
    "    `sparse_f`: per RigL paper, `uniform_sparsity` has fewer FLOPs, `erdos_renyi_sparsity` \n",
    "    results in better model.\n",
    "    \n",
    "    `sparse_init_mode`: initialization mode of sparse modules, or no initialization if None. \n",
    "    Possible values: [None, 'fan_in', 'fan_out', 'fan_in_out']\n",
    "    \n",
    "    If `enforce_mask` is True, a forward_pre_hook will be registered to each module\n",
    "    to apply the weight mask before every forward pass of the module.\n",
    "    \n",
    "    Returns a fastai Hooks object. You can remove the hooks after training by calling hooks.remove().\n",
    "    '''\n",
    "    if isinstance(model, Learner): model = model.model\n",
    "    modules = sparseable_modules(model)\n",
    "    module_name_param = L([(m, p_name, p) for m in modules for p_name, p in m.named_parameters()\n",
    "                         if 'weight' in p_name])\n",
    "    params = module_name_param.itemgot(2)\n",
    "    sparsities = sparse_f(params, model_sparsity)\n",
    "    \n",
    "    hooks = Hooks([], noop)\n",
    "    for (m, p_name, p), s in zip(module_name_param, sparsities):\n",
    "        if s > 0:\n",
    "            mask = sparse_mask_like(m.weight, s)\n",
    "            m.register_buffer('weight_mask', mask)\n",
    "            m.register_buffer('weight_sparsity', tensor(s))\n",
    "            apply_masks(m)\n",
    "            if sparse_init_mode is not None:\n",
    "                init_f = partial(init_kaiming_normal_sparse_, sparse_mode=sparse_init_mode)\n",
    "                init_default(m, func=init_f)\n",
    "                apply_masks(m)\n",
    "            if enforce_mask: \n",
    "                h = m.register_forward_pre_hook(apply_masks)\n",
    "                hooks.hooks.append(h)\n",
    "    \n",
    "    return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "n_params = sum(m.weight.numel() for m in s_mods)\n",
    "sparsify_model(model, 0.9, sparse_f=uniform_sparsity)\n",
    "n_nonzeros = sum(m.weight.abs().gt(0).sum() for m in s_mods)\n",
    "# increase `eps` to account for rounding to nearest whole weight\n",
    "test_close(n_nonzeros, 0.1 * n_params, eps=len(s_mods))\n",
    "p = s_mods[0].weight\n",
    "test_close(p.abs().gt(0).sum(), 0.1 * p.numel(), eps=1)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1,50), nn.ReLU(), nn.Linear(50,1))\n",
    "hooks = sparsify_model(model, 0.9)\n",
    "model(torch.rand(10,1))\n",
    "test_eq(10, sum([model[i].weight.abs().gt(0).sum() for i in (0,2)]))\n",
    "hooks.remove()\n",
    "for i in (0,2): model[i].weight.data = torch.ones_like(model[i].weight)\n",
    "model(torch.rand(10,1))\n",
    "test_eq(100, sum([model[i].weight.abs().gt(0).sum() for i in (0,2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop/Grow Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_score(p, **kwargs): return torch.rand_like(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_magnitude(p, **kwargs): return p.data.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_magnitude(p, **kwargs): return p.grad.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_momentum(p, opt, **kwargs):\n",
    "    '''Calculates the momentum of the gradient for a parameter `p` from the `opt` state.'''\n",
    "    state = opt.state[p]\n",
    "    grad_avg = state['grad_avg'] if 'grad_avg' in state else None\n",
    "    sqr_avg = state['sqr_avg'] if 'sqr_avg' in state else None\n",
    "    if grad_avg is None:\n",
    "        raise Exception(f\"Error: 'grad_avg' key not found in optimizer state. Tip: set the `mom` hyperparamter in the learner.\")\n",
    "    if sqr_avg is None:\n",
    "        grad_mom = grad_avg\n",
    "    else:\n",
    "        try: eps = opt.state_dict()['hypers'][0]['eps']\n",
    "        except: eps = 1e-6\n",
    "        grad_mom =  grad_avg / (torch.sqrt(sqr_avg + eps))\n",
    "    return grad_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity Redistribution Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_redistribution(dst_cb):\n",
    "    '''\n",
    "    Modifies each sparseable parameter's target sparsity proportional to its mean absolute momentum.\n",
    "    \n",
    "    Based on redistribution method in Sparse Networks From Scratch by Dettmers et al. \n",
    "    (https://arxiv.org/abs/1907.04840). Instead of evenly distributing leftover weights, as in the\n",
    "    official implementation, this method finds exact distribution amounts by making parameters dense\n",
    "    one at a time until valid sparsities are found.\n",
    "    '''\n",
    "    param_d = {p: (mask, s, m) for m in dst_cb.modules for p,mask,s in sparse_params(m)}\n",
    "    \n",
    "    # calculate mean absolute momentum per layer and total # of params to distribute\n",
    "    p2mom, p2drop, p2maxgrow = {}, {}, {}\n",
    "    for p, (mask, s, m) in param_d.items():\n",
    "            mom = gradient_momentum(p, dst_cb.learn.opt)\n",
    "            mean_nonzero_mom = (mom * mask).abs().sum() / mask.sum()\n",
    "            p2mom[p] = mean_nonzero_mom\n",
    "            \n",
    "            n_nonzeros = mask.sum()\n",
    "            n_zeros = mask.numel() - n_nonzeros\n",
    "            n_drop = int(n_nonzeros * dst_cb.drop_grow_pct)\n",
    "            \n",
    "            p2drop[p] = n_drop\n",
    "            p2maxgrow[p] = n_zeros + n_drop\n",
    "            \n",
    "    # normalize momentum contributions to determine each parameters's growth factor\n",
    "    total_mom = sum(p2mom.values())\n",
    "    p2growth_factor = {p: float(mom / total_mom) for p, mom in p2mom.items()}\n",
    "    \n",
    "    total_n_drop = sum(p2drop.values())\n",
    "    if total_n_drop == 0:\n",
    "        return\n",
    "    \n",
    "    # Distribute weights proportional to parameter's momentum, without changing overall sparsity\n",
    "    #   total_n_drop     = total_n_grow\n",
    "    #   sum_p: n_drop[p] = sum_p: n_grow[p]\n",
    "    #   sum_p: n_drop[p] = sum_dense_p: max_grow[p]\n",
    "    #                    + eps * sum_sparse_p: growth_factor[p] * n_drop[p]\n",
    "    # Goal is to find eps satisfying ^ this ^ equation where no layer's density > 1:\n",
    "    #   eps = ( sum(n_drop[p]) - sum_dense_p(max_grow[p]) ) / sum_sparse_p(growth_factor[p] * n_drop[p])\n",
    "    #   eps = (total_n_drop - total_dense_grow) / proportional_sparse_grow\n",
    "    # Loop until no target density > 1, adding largest layer to dense set if not satisfied\n",
    "    \n",
    "#     print('dropping:', total_n_drop, 'individ:', p2drop.values())\n",
    "    p2grow = {}\n",
    "    dense_params = set()\n",
    "    done = False\n",
    "    while done == False:\n",
    "        for p, (mask, s, m) in param_d.items():\n",
    "            if p in dense_params:\n",
    "                p2grow[p] = p2maxgrow[p] # = total_dense_grow[p]\n",
    "            else:\n",
    "                p2grow[p] = p2growth_factor[p] * p2drop[p] # = proportional_sparse_grow[p]\n",
    "        \n",
    "        # find eps\n",
    "        total_dense_grow = sum(p2grow[p] for p in param_d.keys() if p in dense_params)\n",
    "        proportional_sparse_grow = sum(p2grow[p] for p in param_d.keys() if p not in dense_params)\n",
    "#         print('dense:', [p.numel() for p in dense_params])\n",
    "        eps = (total_n_drop - total_dense_grow) / proportional_sparse_grow\n",
    "        \n",
    "        # find new sparsities\n",
    "        p2sparsity = {}\n",
    "        for p, (mask, s, m) in param_d.items():\n",
    "            if p in dense_params:\n",
    "                p2sparsity[p] = 0.\n",
    "            else:\n",
    "                n_drop = p2drop[p]\n",
    "                n_grow = eps * p2grow[p]\n",
    "                target_nonzeros = mask.sum() - n_drop + n_grow\n",
    "                p2sparsity[p] = 1 - target_nonzeros / mask.numel()\n",
    "        \n",
    "        # if any sparse params have sparsity < 0 (i.e. denser than possible), move the lowest sparsity\n",
    "        # param to the set of dense params, otherwise end loop\n",
    "        min_sparsity = min([s for s in p2sparsity.values()])\n",
    "        if min_sparsity < 0:\n",
    "            for p, s in p2sparsity.items():\n",
    "                if s == min_sparsity:\n",
    "                    dense_params.add(p)\n",
    "        else:\n",
    "            done = True\n",
    "    \n",
    "    # set each parameter's sparsity buffer to new target sparsity\n",
    "    for p, (mask, s, m) in param_d.items():\n",
    "        pname = {param:pname for pname, param in m.named_parameters()}[p]\n",
    "        sparsity_buffer = getattr(m, pname+'_sparsity')\n",
    "        sparsity_buffer.data = torch.tensor(float(p2sparsity[p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Sparse Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def top_k_mask(t, n_keep):\n",
    "    '''Returns a mask with `n_keep` ones cooresponding to the largest values in `t`'''\n",
    "    n_drop = t.numel() - n_keep\n",
    "    _, sorted_ixs = torch.topk(t.flatten(), k=t.numel())\n",
    "    mask = torch.cat([torch.ones(n_keep, dtype=torch.bool, device=t.device), \n",
    "                      torch.zeros(n_drop, dtype=torch.bool, device=t.device)])\n",
    "    mask = mask.scatter(0, sorted_ixs, mask)\n",
    "    return mask.view(*t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-0.9, 0.9, 20).reshape(4,5)\n",
    "mask = top_k_mask(t, 5)\n",
    "test_eq(0, mask[:3].sum())\n",
    "test_eq(5, mask[3:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DynamicSparseTrainingCallback(Callback):\n",
    "    '''Dynamically updates the network connectivity during training.'''\n",
    "    def __init__(self, sparse_modules=None,\n",
    "                 batches_per_update=None, initial_drop_grow_pct=0.3, stop_pct=0.75, \n",
    "                 keep_score_f=weight_magnitude, grow_score_f=gradient_magnitude, redistribute_f=None):\n",
    "        store_attr('initial_drop_grow_pct,stop_pct,keep_score_f,grow_score_f,redistribute_f,batches_per_update')\n",
    "        self.modules = sparse_modules\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.modules = ifnone(self.modules, sparseable_modules(self.learn.model))\n",
    "        self.batches_per_update = ifnone(self.batches_per_update, len(self.dls.train))\n",
    "        self.drop_grow_pct_sched = combine_scheds(\n",
    "            [self.stop_pct, 1-self.stop_pct],\n",
    "            [SchedCos(self.initial_drop_grow_pct, 0.), SchedNo(0.,0.)]\n",
    "        )\n",
    "        self.n_param_count = sum([int(mask.numel()) for m in self.modules for _,mask,_ in sparse_params(m)])\n",
    "        self.n_nonzeros = sum([int(mask.sum()) for m in self.modules for _,mask,_ in sparse_params(m)])\n",
    "        self.model_sparsity = 1 - self.n_nonzeros / self.n_param_count\n",
    "        \n",
    "    def after_backward(self):\n",
    "        self.step()\n",
    "#         self.learn.opt.step()\n",
    "        if self.is_update_step:\n",
    "            if self.redistribute_f:\n",
    "                self.redistribute_f(self)\n",
    "            for m in self.modules:\n",
    "                self.rewire_module(m)\n",
    "            raise CancelBatchException()\n",
    "        \n",
    "    def step(self):\n",
    "        if not self.training:\n",
    "            self.is_update_step = False\n",
    "        else:\n",
    "            step = self.epoch * self.n_iter + self.iter\n",
    "            n_steps = self.n_epoch * self.n_iter\n",
    "            pct_train = step / n_steps\n",
    "            is_last_step = step + 1 == n_steps\n",
    "            self.is_update_step = (step > 0 \n",
    "                                   and step % self.batches_per_update == 0 \n",
    "                                   and self.drop_grow_pct > 0\n",
    "                                   and not is_last_step)\n",
    "            self.drop_grow_pct = self.drop_grow_pct_sched(pct_train)\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def rewire_module(self, m):\n",
    "        for param, mask, target_sparsity in sparse_params(m):\n",
    "\n",
    "            current_sparsity = 1 - float(mask.sum() / mask.numel())\n",
    "            n_grow = int(mask.sum() * self.drop_grow_pct)\n",
    "            n_keep = mask.sum() - n_grow\n",
    "            \n",
    "#             modify n_grow if actual sparsity differs from target sparsity\n",
    "            current_nonzeros = int(mask.sum())\n",
    "            target_nonzeros = round(mask.numel() * (1 - target_sparsity))\n",
    "\n",
    "            n_grow = max(0, n_grow + target_nonzeros - current_nonzeros)\n",
    "                \n",
    "            # determine which weights to keep\n",
    "            if current_sparsity > 0 and target_sparsity > 0:\n",
    "                keep_score = self.keep_score_f(param, opt=self.learn.opt)\n",
    "                keep_mask = top_k_mask(keep_score, n_keep)\n",
    "            else:\n",
    "                keep_mask = torch.ones_like(mask)\n",
    "\n",
    "            # determine which weights to grow, if any\n",
    "            if self.grow_score_f:\n",
    "                grow_score = self.grow_score_f(param, opt=self.learn.opt)\n",
    "                # make all keep weights to negative so we don't choose to grow them\n",
    "                grow_score = grow_score * keep_mask.logical_not() - keep_mask.float()\n",
    "                grow_mask = top_k_mask(grow_score, n_grow)\n",
    "            else:\n",
    "                grow_mask = torch.zeros_like(mask)\n",
    "                \n",
    "            # update network connectivity\n",
    "            mask.data = keep_mask | grow_mask\n",
    "            \n",
    "            # zero momentum for new connections\n",
    "            self.reset_momentum(param, grow_mask & keep_mask.logical_not())\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def reset_momentum(self, p, mask):\n",
    "        state = self.opt.state[p]\n",
    "        if 'grad_avg' in state: state['grad_avg'].mul_(mask)\n",
    "        if 'sqr_avg' in state: state['sqr_avg'].mul_(mask)\n",
    "\n",
    "    _docs = dict(__init__='''Args:\n",
    "    sparse_modules: optional, specify which modules to modify the connectivity of\n",
    "    batches_per_update: # of batches per update, None (default) updates at end of each training epoch\n",
    "    initial_drop_grow_pct: percentage of weights to change during each dynamic weight update\n",
    "    stop_pct: stop dynamic weight updates after `stop_pct` of training\n",
    "    keep_score_f: function scoring each weight, top n are kept and the rest are zeroed\n",
    "    grow_score_f: function scoring each weight, top n excl. kept weights are unmasked and initialized to zero''',\n",
    "                 before_fit=\"Schedule the number of connections to drop & grow per update.\",\n",
    "                 before_batch=\"Add dynamic update hooks.\",\n",
    "                 after_backward=\"Remove dynamic update hooks and skip gradient update.\",\n",
    "                 step=\"Update self.is_update_step and self.drop_grow_pct.\",\n",
    "                 rewire_module=\"Update step for one module.\",\n",
    "                 reset_momentum=\"Initialize momentum to zero for newly-added connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"DynamicSparseTrainingCallback\" class=\"doc_header\"><code>class</code> <code>DynamicSparseTrainingCallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>DynamicSparseTrainingCallback</code>(**`sparse_modules`**=*`None`*, **`batches_per_update`**=*`None`*, **`initial_drop_grow_pct`**=*`0.3`*, **`stop_pct`**=*`0.75`*, **`keep_score_f`**=*`weight_magnitude`*, **`grow_score_f`**=*`gradient_magnitude`*, **`redistribute_f`**=*`None`*) :: `Callback`\n",
       "\n",
       "Dynamically updates the network connectivity during training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DynamicSparseTrainingCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test the callback on a toy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.035391</td>\n",
       "      <td>3.574415</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.912396</td>\n",
       "      <td>1.998781</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.365177</td>\n",
       "      <td>0.549359</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.372995</td>\n",
       "      <td>0.178068</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.729602</td>\n",
       "      <td>0.098766</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.302802</td>\n",
       "      <td>0.090962</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.003140</td>\n",
       "      <td>0.042220</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.782096</td>\n",
       "      <td>0.027155</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.616368</td>\n",
       "      <td>0.020773</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.490504</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(1,32), nn.ReLU(), nn.Linear(32,32), nn.ReLU(), nn.Linear(32,1))\n",
    "learn = synth_learner(data=synth_dbunch(bs=100), model=model)\n",
    "sparse_hooks = sparsify_model(learn.model, 0.8, sparse_f=first_layer_dense_uniform)\n",
    "cbs = DynamicSparseTrainingCallback(redistribute_f=momentum_redistribution, batches_per_update=None, stop_pct=0.9, grow_score_f=gradient_momentum)\n",
    "learn.fit(10, lr=1e-2, cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test a slightly more realistic use case: MNIST_TINY on ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.287880</td>\n",
       "      <td>0.303525</td>\n",
       "      <td>0.865522</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184923</td>\n",
       "      <td>0.368318</td>\n",
       "      <td>0.914163</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.141438</td>\n",
       "      <td>0.828287</td>\n",
       "      <td>0.804006</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100892</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.997139</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.074458</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.997139</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "\n",
    "# without momentum_redistribution\n",
    "dls = ImageDataLoaders.from_folder(untar_data(URLs.MNIST_TINY))\n",
    "learn = Learner(dls, xresnet18(n_out=2), metrics=accuracy)\n",
    "sparse_hooks = sparsify_model(learn.model, 0.9, erdos_renyi_sparsity)\n",
    "cbs = DynamicSparseTrainingCallback(batches_per_update=8, stop_pct=0.9, \n",
    "                                    grow_score_f=gradient_momentum)\n",
    "learn.fit_one_cycle(5, 1e-2, cbs=cbs)\n",
    "\n",
    "test_close(1, learn.final_record[-1], eps=0.03) # better than 97% accuracy\n",
    "\n",
    "for m in sparseable_modules(learn.model):\n",
    "    for p, mask, s in sparse_params(m):\n",
    "        n_alive = p.abs().gt(0).sum()\n",
    "        n_total = p.numel()    \n",
    "        test_close(s, 1 - n_alive / n_total, eps=0.01) # layer sparsity = target sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.679205</td>\n",
       "      <td>0.693113</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.411124</td>\n",
       "      <td>0.627829</td>\n",
       "      <td>0.499285</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281064</td>\n",
       "      <td>0.229472</td>\n",
       "      <td>0.915594</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.200176</td>\n",
       "      <td>0.191637</td>\n",
       "      <td>0.989986</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146041</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>0.998569</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "\n",
    "# with momentum_redistribution\n",
    "dls = ImageDataLoaders.from_folder(untar_data(URLs.MNIST_TINY))\n",
    "learn = Learner(dls, xresnet18(n_out=2), metrics=accuracy)\n",
    "sparse_hooks = sparsify_model(learn.model, 0.99, first_layer_dense_uniform)\n",
    "cbs = DynamicSparseTrainingCallback(batches_per_update=8, stop_pct=0.9, \n",
    "                                    grow_score_f=gradient_momentum, \n",
    "                                    redistribute_f=momentum_redistribution)\n",
    "\n",
    "n_nonzeros = sum([mask.sum() for m in sparseable_modules(learn.model) for p, mask, s in sparse_params(m)])\n",
    "n = sum([mask.numel() for m in sparseable_modules(learn.model) for p, mask, s in sparse_params(m)])\n",
    "before_sparsity = n_nonzeros / n\n",
    "\n",
    "learn.fit_one_cycle(5, 1e-2, cbs=cbs)\n",
    "\n",
    "test_close(1, learn.final_record[-1], eps=0.03) # better than 97% accuracy\n",
    "\n",
    "n_nonzeros = sum([mask.sum() for m in sparseable_modules(learn.model) for p, mask, s in sparse_params(m)])\n",
    "n = sum([mask.numel() for m in sparseable_modules(learn.model) for p, mask, s in sparse_params(m)])\n",
    "after_sparsity = n_nonzeros / n\n",
    "\n",
    "test_close(before_sparsity, after_sparsity, eps=1e-5) # model sparsity is unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preset Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Evolutionary Training (SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SET_presets = {'keep_score_f': weight_magnitude, 'grow_score_f': random_score, \n",
    "               'initial_drop_grow_pct': 0.3, 'stop_pct': 1.0,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Networks From Scratch (SNFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SNFS_presets = {'redistribute_f':momentum_redistribution,\n",
    "                'keep_score_f': weight_magnitude, 'grow_score_f': gradient_momentum, \n",
    "                'initial_drop_grow_pct': 0.5, 'stop_pct': 1.0,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rigged Lottery (RigL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RigL_presets = {'keep_score_f': weight_magnitude, 'grow_score_f': gradient_magnitude, \n",
    "                'initial_drop_grow_pct':0.3, 'stop_pct':0.75, 'batches_per_update': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
