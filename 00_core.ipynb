{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Core\n",
    "\n",
    "> Basic functions for sparsifying dense modules & models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import * # L\n",
    "from fastai.basics import * # flatten_model, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Module\n",
    "\n",
    "> For sparsifying a single module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a parameter and buffer in a module follow the naming convention: `{p_name}`, `{p_name}_mask`, respectively, the buffer is assumed to be the mask for the parameter. For example, masked Linear and ConvNd layers will typically have a parameter named `weight` and a buffer named `weight_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparse_mask(sizes, sparsity):\n",
    "    n_total = np.prod(sizes)\n",
    "    n_ones = round((1-sparsity) * n_total)\n",
    "    shuffled_ones = torch.randperm(n_total)[:n_ones]\n",
    "    mask = torch.zeros(n_total, dtype=torch.bool)\n",
    "    mask[shuffled_ones] = True\n",
    "    return mask.reshape(*sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sparse_mask((10,5), 0.8)\n",
    "test_eq(10, int(mask.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def masked_params(module):\n",
    "    '''Returns list of (param, mask) tuples, assuming masks are buffers with name scheme: {param}_mask.'''\n",
    "    buffer_d = {name:b for name, b in module.named_buffers()}\n",
    "    param_mask_pairs = [(p, buffer_d[f'{name}_mask']) \n",
    "                        for name,p in module.named_parameters() \n",
    "                        if f'{name}_mask' in buffer_d]\n",
    "    return param_mask_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(5,10)\n",
    "m.register_buffer('weight_mask', mask)\n",
    "param_mask_pairs = masked_params(m)\n",
    "test_eq(param_mask_pairs[0][0], m.weight)\n",
    "test_eq(param_mask_pairs[0][1], m.weight_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def apply_masks(module, *args, inplace=True):\n",
    "    for p, mask in masked_params(module): \n",
    "        if inplace: p.data.mul_(mask)\n",
    "        else:       p.data = p.data.mul(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_masks(m)\n",
    "test_eq(10, m.weight.abs().gt(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_sparseable_module_types = nn.Linear, nn.Conv2d\n",
    "\n",
    "def is_sparseable_module(m, additional_types=[]):\n",
    "    types = set(_sparseable_module_types)\n",
    "    if additional_types: types |= set(additional_types)\n",
    "    return isinstance(m, tuple(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sparseable_modules(model, additional_types=[]):\n",
    "    return [m for m in flatten_model(model) if is_sparseable_module(m, additional_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3,32,3), nn.ReLU(), \n",
    "        nn.Conv2d(32,128,3), nn.ReLU(), \n",
    "        nn.Conv2d(128,512,3), nn.ReLU(), Flatten(),\n",
    "        nn.Linear(512, 10))\n",
    "\n",
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "test_eq(4, len(s_mods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Distributions\n",
    "\n",
    "> For determining the layer-wise sparsity of a list of modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "> All layers have a the same percentage of connection removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_sparsity(params, model_sparsity):\n",
    "    return [model_sparsity] * len(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Layer-Dense Uniform Distribution\n",
    "\n",
    "> Uniform sparsity except for the first layer, which is dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def first_layer_dense_uniform(params, model_sparsity):\n",
    "    sparsities = [1.] + [model_sparsity] * (len(params) - 1)\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erdos-Renyi (Kernel) Distribution\n",
    "\n",
    "> For a fixed overall sparsity, the Erdos-Renyi sparsity distribution allocates more connections to smaller layers and fewer to large layers when compared to a uniform sparsity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# modified from https://github.com/google-research/rigl/blob/master/rigl/sparse_utils.py.\n",
    "def erdos_renyi_sparsity(params, model_sparsity, include_kernel=True, erk_power_scale=1.0):\n",
    "    \"\"\"\n",
    "    Returns a list of sparsities in the same order as params. Sparsities satisfy \n",
    "    the Erdos-Renyi(Kernel) distribution, where the model has a total parameter count \n",
    "    as one with uniform sparsities, that is, satisfying the following equation:\n",
    "    # eps * (p_1 * N_1 + p_2 * N_2) = (1 - model_sparsity) * (N_1 + N_2), for some float `eps`.\n",
    "    \n",
    "    Args:\n",
    "    params: list of all sparseable parameters\n",
    "    model_sparsity: target overall sparsity between 0 and 1\n",
    "    include_kernel: if True, kernel dimensions are included in the scaling (e.g. for ConvNd layers)\n",
    "    erk_power_scale: scale < 1 softens the erdos_renyi distribution (i.e. closer to uniform)\n",
    "    \n",
    "    Returns a list of sparsities where values correspond to individual param sparsities.\n",
    "    \"\"\"\n",
    "    # Enforce custom sparsities, then find correct scaling factor, `eps` for remaining params\n",
    "    dense_layers = set()\n",
    "    is_eps_valid = False\n",
    "    while not is_eps_valid:\n",
    "        # Start with all layers and try to find right eps. If any sparsity exceeds 1, \n",
    "        # make that layer dense and repeat with the non-dense layers.\n",
    "        #\n",
    "        # E.g. where N_3, and N_4 are found to be dense:\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) + (N_3 + N_4) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2 + N_3 + N_4)\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2) - model_sparsity * (N_3 + N_4) <--- == rhs\n",
    "        # eps = rhs / (\\sum_i p_i * N_i) <--- == divisor\n",
    "        # eps = rhs / divisor\n",
    "\n",
    "        divisor = 0\n",
    "        rhs = 0\n",
    "        raw_sparsity = {}\n",
    "        for p in params:\n",
    "            n_zeros = int(np.floor(model_sparsity * p.numel()))\n",
    "            if p in dense_layers:\n",
    "                rhs -= n_zeros\n",
    "            else:\n",
    "                n_ones = p.numel() - n_zeros\n",
    "                rhs += n_ones\n",
    "                if include_kernel:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape) / np.prod(p.shape))**erk_power_scale\n",
    "                else:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape[:2]) / np.prod(p.shape[:2]))\n",
    "                divisor += raw_sparsity[p] * p.numel()\n",
    "                \n",
    "        eps = rhs / divisor\n",
    "        \n",
    "        # If eps * raw_sparsity[p] > 1, we add the param to the set of dense_layers\n",
    "        max_sparsity = np.max(list(raw_sparsity.values()))\n",
    "        if eps * max_sparsity > 1:\n",
    "            for p, p_raw_sparsity in raw_sparsity.items():\n",
    "                if p_raw_sparsity == max_sparsity:\n",
    "                    dense_layers.add(p)\n",
    "        else:\n",
    "            is_eps_valid = True\n",
    "\n",
    "    # With the valid eps, we can set sparsities of the remaining layers\n",
    "    sparsities = [0. if p in dense_layers else (1. - eps * raw_sparsity[p]) for p in params]\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_params = L(sparseable_modules(model)).map(lambda m: m.weight)\n",
    "\n",
    "sparsities = erdos_renyi_sparsity(s_params, 0.9)\n",
    "n_nonzeros = sum([(1-s) * p.numel() for p, s in zip(s_params, sparsities)])\n",
    "test_close(n_nonzeros, 0.1 * sum([p.numel() for p in s_params]), eps=len(s_params))\n",
    "# test_eq([0., 0., 0., 0.], sparsities) # TODO: calc sparsities by hand and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Model\n",
    "\n",
    "> For sparsifying an entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparsify_model(model, model_sparsity, sparse_init_f=uniform_sparsity, enforce_mask=True):\n",
    "    '''\n",
    "    Adds a sparse mask for each sparseable-module weight in model and applies mask to weights.\n",
    "    \n",
    "    If `enforce_mask` is True, a forward_pre_hook will be registered to each module\n",
    "    to apply the weight mask before every forward pass of the module.\n",
    "    \n",
    "    `sparsify_method`: per RigL paper, `uniform_sparsity` has fewer FLOPs, `erdos_renyi_sparsity` \n",
    "    results in better model.\n",
    "    \n",
    "    Returns hooks if `enforce_mask` == True, otherwise None.\n",
    "    '''\n",
    "    sparseable_modules = L(model.modules()).filter(is_sparseable_module)\n",
    "    sparseable_params = sparseable_modules.map(lambda m: m.weight)\n",
    "    sparsities = sparse_init_f(sparseable_params, model_sparsity)\n",
    "    hooks = []\n",
    "    for m, s in zip(sparseable_modules, sparsities):\n",
    "        if s > 0:\n",
    "            mask = sparse_mask(m.weight.shape, s).to(m.weight.device)\n",
    "            m.register_buffer('weight_mask', mask)\n",
    "            apply_masks(m)\n",
    "            if enforce_mask: \n",
    "                h = m.register_forward_pre_hook(apply_masks)\n",
    "                hooks.append(h)\n",
    "    return hooks or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "n_params = sum(m.weight.numel() for m in s_mods)\n",
    "sparsify_model(model, 0.9, sparse_init_f=uniform_sparsity)\n",
    "n_nonzeros = sum(m.weight.abs().gt(0).sum() for m in s_mods)\n",
    "# increase `eps` to account for rounding to nearest whole weight\n",
    "test_close(n_nonzeros, 0.1 * n_params, eps=len(s_mods))\n",
    "p = s_mods[0].weight\n",
    "test_close(p.abs().gt(0).sum(), 0.1 * p.numel(), eps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop/Grow Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_magnitude(p, *args): return p.data.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_magnitude(p, *args): return p.grad.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_momentum(p, opt, *args):\n",
    "    '''Calculates the momentum of the gradient for a parameter `p` from the `opt` state.'''\n",
    "    state = opt.state[p]\n",
    "    grad_avg = state['grad_avg'] if 'grad_avg' in state else None\n",
    "    sqr_avg = state['sqr_avg'] if 'sqr_avg' in state else None\n",
    "    if grad_avg is None:\n",
    "        raise Exception(f\"Error: 'grad_avg' key not found in optimizer state. Tip: set the `mom` hyperparamter in the learner.\")\n",
    "    if sqr_avg is None:\n",
    "        grad_mom = grad_avg\n",
    "    else:\n",
    "        try: eps = opt.state_dict()['hypers'][0]['eps']\n",
    "        except: eps = 1e-6\n",
    "        print(eps)\n",
    "        grad_mom =  grad_avg / (torch.sqrt(sqr_avg + eps))\n",
    "    return grad_mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Sparse Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def top_k_mask(t, n_keep):\n",
    "    '''Returns a mask with `n_keep` ones cooresponding to the largest values in `t`'''\n",
    "    n_drop = t.numel() - n_keep\n",
    "    _, sorted_ixs = torch.topk(t.flatten(), k=t.numel())\n",
    "    mask = torch.cat([torch.ones(n_keep, dtype=torch.bool, device=t.device), \n",
    "                      torch.zeros(n_drop, dtype=torch.bool, device=t.device)])\n",
    "    mask = mask.scatter(0, sorted_ixs, mask)\n",
    "    return mask.view(*t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-0.9, 0.9, 20).reshape(4,5)\n",
    "mask = top_k_mask(t, 5)\n",
    "test_eq(0, mask[:3].sum())\n",
    "test_eq(5, mask[3:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DynamicSparseTrainingCallback(Callback):\n",
    "    toward_end = True # run after GradientAccumulation and any other cb that modifies the gradients\n",
    "    _exclude_modules = [nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]\n",
    "    _sparse_distributions = ('uniform', 'ERK')\n",
    "    \n",
    "    def __init__(self, sparsity=0.9, modules=None, sparsity_distribution='uniform', \n",
    "                 batches_per_update=None, \n",
    "                 initial_drop_grow_pct=0.3, stop_pct=0.75, \n",
    "                 keep_method=weight_magnitude,\n",
    "                 grow_method=gradient_magnitude,\n",
    "                 exclude_modules=[], first_layer_dense=True):\n",
    "        store_attr('sparsity,modules,sparsity_distribution,initial_drop_grow_pct,stop_pct,keep_method,grow_method,first_layer_dense')\n",
    "        self.batches_per_update = ifnone(batches_per_update, len(self.dls.train)) # default: 1 update per epoch\n",
    "        self.exclude_modules = exclude_modules + self._exclude_modules\n",
    "        \n",
    "    def before_fit(self):\n",
    "        ### determine modules to sparsify\n",
    "        is_sparse_module = lambda m: has_params(m) and hasattr(m, 'weight') and type(m) not in self.exclude_modules\n",
    "        self.modules = ifnone(self.modules, [m for m in flatten_model(self.learn.model) if is_sparse_module(m)])\n",
    "\n",
    "        ### determine initial sparsities per layer\n",
    "        assert self.sparsity_distribution in self._sparse_distributions, f'Unknown sparsity distribution: {self.sparsity_distribution}. Options: {self._sparse_distributions}'\n",
    "        if self.sparsity_distribution == 'uniform':\n",
    "            self.S = [self.sparsity] * len(self.modules)\n",
    "            if self.first_layer_dense:\n",
    "                self.S[0] = 0\n",
    "        elif self.sparsity_distribution == 'ERK':\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        if self.grow_method not in self._grow_methods: assert is_function(self.grow_method)\n",
    "        \n",
    "        ### create masks and assign to each parameter\n",
    "        for m, s in zip(self.modules, self.S):\n",
    "            if s > 0: m.register_buffer('weight_mask', (torch.rand_like(m.weight) > s).bool())\n",
    "            \n",
    "        ### schedule the decay percent (i.e. # of connections to drop/add per update)\n",
    "        self.drop_grow_pct_sched = combine_scheds([self.stop_pct, 1-self.stop_pct], \n",
    "                                                  [SchedCos(self.initial_drop_grow_pct, 0.), SchedNo(0.,0.)])\n",
    "        \n",
    "        ### apply weight masks\n",
    "        self.add_hooks()\n",
    "        \n",
    "    def after_fit(self):\n",
    "        self.remove_hooks() # ensure hooks are removed (e.g. in case we cancelled the fit loop)\n",
    "        \n",
    "    def add_hooks(self):\n",
    "        self.remove_hooks() # to be certain that we never add them twice\n",
    "        self.hooks = [m.register_forward_pre_hook(apply_masks) for m in self.modules]\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        if getattr(self, 'hooks', None):\n",
    "            for h in self.hooks: h.remove()\n",
    "    \n",
    "    def before_batch(self):\n",
    "        if not self.training: return\n",
    "        if self.is_update_step():\n",
    "#             print(f'UPDATE step! before_batch, epoch: {self.learn.epoch}, step: {self.learn.iter}')\n",
    "            self.hooks = Hooks(self.modules, self.rewire_module, is_forward=False)\n",
    "    def after_backward(self):\n",
    "        if self.is_update_step():\n",
    "#             print(f'UPDATE step! after_backward, epoch: {self.learn.epoch}, step: {self.learn.iter}')\n",
    "            self.hooks.remove()\n",
    "            \n",
    "            ### skip gradient update after changing network connectivity\n",
    "            raise CancelBatchException()\n",
    "        \n",
    "    def is_update_step(self):\n",
    "        '''Whether to modify network connectivity. Side effect: updates self.drop_grow_pct'''\n",
    "        step = self.epoch * self.n_iter + self.iter\n",
    "        n_steps = self.n_epoch * self.n_iter\n",
    "        pct_train = step / n_steps\n",
    "        self.drop_grow_pct = self.drop_grow_pct_sched(pct_train)\n",
    "        return step > 0 and step % self.batches_per_update == 0 and self.drop_grow_pct > 0\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def rewire_module(self, m, *args):\n",
    "        '''Update step for one module'''\n",
    "        \n",
    "        for m, s in zip(self.modules, self.S):\n",
    "            if s <= 0: continue # ignore fully dense layers\n",
    "            \n",
    "            param, mask = m.weight, m.weight_mask\n",
    "            \n",
    "            ### determine # of connections to keep, # to regrow\n",
    "            n_keep = self.compute_n_keep(s, param, mask)\n",
    "            n_grow = self.compute_n_grow(s, param, mask)\n",
    "            \n",
    "            ### determine weights to keep\n",
    "            keep_score = self.keep_method(p)\n",
    "            keep_mask = top_k_mask(keep_score, n_keep)\n",
    "            \n",
    "            ### determine weights to grow\n",
    "            grow_score = self.grow_method(p, self.learn.opt)\n",
    "            # set keep weights to negative so we don't choose to grow them\n",
    "            grow_score = grow_score * keep_mask.logical_not() - keep_mask.float()\n",
    "            grow_mask = top_k_mask(grow_score, n_grow)\n",
    "            \n",
    "            ### update mask\n",
    "            mask.data = keep_mask | grow_mask\n",
    "            \n",
    "            ### zero momentum for new connections\n",
    "            self.reset_momentum(param, grow_mask & keep_mask.logical_not())\n",
    "\n",
    "    def compute_n_grow(self, s, p, mask):\n",
    "        return (1 - s) * p.numel() * self.drop_grow_pct\n",
    "    def compute_n_keep(self, s, p, mask):\n",
    "        return p.numel() - self.compute_n_grow()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_momentum(self, p, grow_mask):\n",
    "        '''Initialize momentum to zero for newly-added connections'''\n",
    "        state = self.opt.state[p]\n",
    "        if 'grad_avg' in state: state['grad_avg'].mul_(grow_mask)\n",
    "        if 'sqr_avg' in state: state['sqr_avg'].mul_(grow_mask)\n",
    "\n",
    "#     _docs = dict(before_fit=\"Set counter to 0\",\n",
    "#                  after_backward=\"Skip weight update if we have not seen enough items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Evolutionary Training (SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SET_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Training From Scratch (SNFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "STFS_kwargs = {'keep_method': weight_magnitude, 'grow_method': gradient_momentum, \n",
    "               'batches_per_update': None, 'initial_drop_grow_pct': 0.3, 'stop_pct': 1.0,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rigged Lottery (RigL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RigL_kwargs = {'keep_method': weight_magnitude, 'grow_method': gradient_magnitude, \n",
    "               'batches_per_update':None, 'initial_drop_grow_pct':0.3, 'stop_pct':0.75,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
