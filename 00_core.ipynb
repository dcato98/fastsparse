{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Core\n",
    "\n",
    "> Core functionality for sparsifying dense modules & models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import * # L, etc...\n",
    "from fastai.basics import * # flatten_model, etc...\n",
    "from fastai.callback.all import * # combine_scheds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Module\n",
    "\n",
    "> For sparsifying a single module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a parameter and buffer in a module follow the naming convention: `{p_name}`, `{p_name}_mask`, respectively, the buffer is assumed to be the mask for the parameter. For example, masked Linear and ConvNd layers will typically have a parameter named `weight`, a buffer named `weight_mask`. Additionally, parameters optionally also contain a sparsity buffer (e.g. for ConvNd, named `weight_sparsity`), which is used by the DynamicSparseTrainingCallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparse_mask(sizes, sparsity):\n",
    "    n_total = np.prod(sizes)\n",
    "    n_ones = round((1-sparsity) * n_total)\n",
    "    shuffled_ones = torch.randperm(n_total)[:n_ones]\n",
    "    mask = torch.zeros(n_total, dtype=torch.bool)\n",
    "    mask[shuffled_ones] = True\n",
    "    return mask.reshape(*sizes)\n",
    "\n",
    "def sparse_mask_like(param, sparsity): return sparse_mask(param.shape, sparsity).to(param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sparse_mask((10,5), 0.8)\n",
    "test_eq(10, int(mask.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maybe_float(num):\n",
    "    try: return float(num)\n",
    "    except: return num\n",
    "    \n",
    "def sparse_params(module):\n",
    "    '''Returns list of all (param, mask, sparsity) tuples in a module.'''\n",
    "    buffer_d = {name:b for name, b in module.named_buffers()}\n",
    "    param_mask_sparsities = [(p, buffer_d[f'{name}_mask'], maybe_float(buffer_d.get(f'{name}_sparsity')))\n",
    "                             for name, p in module.named_parameters() \n",
    "                             if f'{name}_mask' in buffer_d]\n",
    "    return list(set(param_mask_sparsities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = 0.8, nn.Linear(5,10)\n",
    "m.register_buffer('weight_mask', sparse_mask_like(m.weight, s))\n",
    "m.register_buffer('weight_sparsity', tensor(s))\n",
    "m.register_buffer('bias_mask', sparse_mask_like(m.bias, s))\n",
    "param_mask_sparsity = sparse_params(m)\n",
    "test_eq(2, len(param_mask_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def apply_masks(module, *args, inplace=True):\n",
    "    for param, mask, sparsity in sparse_params(module):\n",
    "        if inplace: param.data.mul_(mask)\n",
    "        else:       param.data = param.data.mul(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_masks(m)\n",
    "test_eq(10, m.weight.abs().gt(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_sparseable_module_types = (nn.Linear, \n",
    "                            nn.Conv1d, nn.Conv2d, nn.Conv3d, \n",
    "                            nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,\n",
    "                            nn.MultiheadAttention,\n",
    "                            nn.RNN, nn.RNNCell, nn.GRU, nn.GRUCell, nn.LSTM, nn.LSTMCell)\n",
    "\n",
    "def is_sparseable_module(m, additional_types=[]):\n",
    "    types = set(_sparseable_module_types) | set(additional_types)\n",
    "    return isinstance(m, tuple(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# TODO: flatten_model gets rid of nn.MultiheadAttention which has it's own parameter 'in_proj_weight'\n",
    "#       which means sparsity_model doesn't sparsify this parameter\n",
    "def sparseable_modules(model, additional_types=[]):\n",
    "    filt = partial(is_sparseable_module, additional_types=additional_types)\n",
    "    return L(flatten_model(model)).filter(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3,32,3), nn.ReLU(), \n",
    "        nn.Conv2d(32,128,3), nn.ReLU(), \n",
    "        nn.Conv2d(128,512,3), nn.ReLU(), AdaptiveAvgPool(), Flatten(),\n",
    "        nn.Linear(512, 10))\n",
    "\n",
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "test_eq(4, len(s_mods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Distributions\n",
    "\n",
    "> For determining the layer-wise sparsity of a list of modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "> All layers have a the same percentage of connection removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_sparsity(params, model_sparsity):\n",
    "    return [model_sparsity] * len(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Layer-Dense Uniform Distribution\n",
    "\n",
    "> Uniform sparsity except for the first layer, which is dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def first_layer_dense_uniform(params, model_sparsity):\n",
    "    sparsities = [0.] + [model_sparsity] * (len(params) - 1)\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erdos-Renyi (Kernel) Distribution\n",
    "\n",
    "> For a fixed overall sparsity, the Erdos-Renyi sparsity distribution allocates more connections to smaller layers and fewer to large layers when compared to a uniform sparsity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# modified from https://github.com/google-research/rigl/blob/master/rigl/sparse_utils.py.\n",
    "def erdos_renyi_sparsity(params, model_sparsity, include_kernel=True, erk_power_scale=1.0):\n",
    "    \"\"\"\n",
    "    Returns a list of sparsities in the same order as params. Sparsities satisfy \n",
    "    the Erdos-Renyi(Kernel) distribution, where the model has a total parameter count \n",
    "    as one with uniform sparsities, that is, satisfying the following equation:\n",
    "    $ eps * (p_1 * N_1 + p_2 * N_2) = (1 - model_sparsity) * (N_1 + N_2) $, for some float `eps`.\n",
    "    \n",
    "    Args:\n",
    "    params: list of all sparseable parameters\n",
    "    model_sparsity: target overall sparsity between 0 and 1\n",
    "    include_kernel: if True, kernel dimensions are included in the scaling (e.g. for ConvNd layers)\n",
    "    erk_power_scale: scale < 1 softens the erdos_renyi distribution (i.e. closer to uniform)\n",
    "    \n",
    "    Returns a list of sparsities where values correspond to individual param sparsities.\n",
    "    \"\"\"\n",
    "    # Enforce custom sparsities, then find correct scaling factor, `eps` for remaining params\n",
    "    dense_layers = set()\n",
    "    is_eps_valid = False\n",
    "    while not is_eps_valid:\n",
    "        # Start with all layers and try to find right eps. If any sparsity exceeds 1, \n",
    "        # make that layer dense and repeat with the non-dense layers.\n",
    "        #\n",
    "        # E.g. where N_3, and N_4 are found to be dense:\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) + (N_3 + N_4) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2 + N_3 + N_4)\n",
    "        # eps * (p_1 * N_1 + p_2 * N_2) =\n",
    "        #    (1 - model_sparsity) * (N_1 + N_2) - model_sparsity * (N_3 + N_4) <--- == rhs\n",
    "        # eps = rhs / (\\sum_i p_i * N_i) <--- == divisor\n",
    "        # eps = rhs / divisor\n",
    "\n",
    "        divisor = 0\n",
    "        rhs = 0\n",
    "        raw_sparsity = {}\n",
    "        for p in params:\n",
    "            n_zeros = int(np.floor(model_sparsity * p.numel()))\n",
    "            if p in dense_layers:\n",
    "                rhs -= n_zeros\n",
    "            else:\n",
    "                n_ones = p.numel() - n_zeros\n",
    "                rhs += n_ones\n",
    "                if include_kernel:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape) / np.prod(p.shape))**erk_power_scale\n",
    "                else:\n",
    "                    raw_sparsity[p] = (np.sum(p.shape[:2]) / np.prod(p.shape[:2]))\n",
    "                divisor += raw_sparsity[p] * p.numel()\n",
    "                \n",
    "        eps = rhs / divisor\n",
    "        \n",
    "        # If eps * raw_sparsity[p] > 1, we add the param to the set of dense_layers\n",
    "        max_sparsity = np.max(list(raw_sparsity.values()))\n",
    "        if eps * max_sparsity > 1:\n",
    "            for p, p_raw_sparsity in raw_sparsity.items():\n",
    "                if p_raw_sparsity == max_sparsity:\n",
    "                    dense_layers.add(p)\n",
    "        else:\n",
    "            is_eps_valid = True\n",
    "\n",
    "    # With the valid eps, we can set sparsities of the remaining layers\n",
    "    sparsities = [0. if p in dense_layers else (1. - eps * raw_sparsity[p]) for p in params]\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_params = L(sparseable_modules(model)).map(lambda m: m.weight)\n",
    "\n",
    "sparsities = erdos_renyi_sparsity(s_params, 0.9)\n",
    "n_nonzeros = sum([(1-s) * p.numel() for p, s in zip(s_params, sparsities)])\n",
    "test_close(n_nonzeros, 0.1 * sum([p.numel() for p in s_params]), eps=len(s_params))\n",
    "# test_eq([0., 0., 0., 0.], sparsities) # TODO: calc sparsities by hand and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsify Model\n",
    "\n",
    "> For sparsifying an entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.no_grad()\n",
    "def sparsify_model(model, model_sparsity, sparse_f=uniform_sparsity, enforce_mask=True):\n",
    "    '''\n",
    "    Adds a sparse mask for each sparseable-module weight in model and applies mask to weights.\n",
    "    \n",
    "    If `enforce_mask` is True, a forward_pre_hook will be registered to each module\n",
    "    to apply the weight mask before every forward pass of the module.\n",
    "    \n",
    "    `sparsify_method`: per RigL paper, `uniform_sparsity` has fewer FLOPs, `erdos_renyi_sparsity` \n",
    "    results in better model.\n",
    "    \n",
    "    Returns a fastai Hooks object. You can remove the hooks after training by calling hooks.remove().\n",
    "    '''\n",
    "    if isinstance(model, Learner): model = model.model\n",
    "    modules = sparseable_modules(model)\n",
    "    module_name_param = L([(m, p_name, p) for m in modules for p_name, p in m.named_parameters()\n",
    "                         if 'weight' in p_name])\n",
    "    params = module_name_param.itemgot(2)\n",
    "    sparsities = sparse_f(params, model_sparsity)\n",
    "    \n",
    "    hooks = Hooks([], noop)\n",
    "    for (m, p_name, p), s in zip(module_name_param, sparsities):\n",
    "        if s > 0:\n",
    "            mask = sparse_mask_like(m.weight, s)\n",
    "            m.register_buffer('weight_mask', mask)\n",
    "            m.register_buffer('weight_sparsity', tensor(s))\n",
    "            apply_masks(m)\n",
    "            if enforce_mask: \n",
    "                h = m.register_forward_pre_hook(apply_masks)\n",
    "                hooks.hooks.append(h)\n",
    "    \n",
    "    return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model()\n",
    "s_mods = sparseable_modules(model)\n",
    "n_params = sum(m.weight.numel() for m in s_mods)\n",
    "sparsify_model(model, 0.9, sparse_f=uniform_sparsity)\n",
    "n_nonzeros = sum(m.weight.abs().gt(0).sum() for m in s_mods)\n",
    "# increase `eps` to account for rounding to nearest whole weight\n",
    "test_close(n_nonzeros, 0.1 * n_params, eps=len(s_mods))\n",
    "p = s_mods[0].weight\n",
    "test_close(p.abs().gt(0).sum(), 0.1 * p.numel(), eps=1)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1,50), nn.ReLU(), nn.Linear(50,1))\n",
    "hooks = sparsify_model(model, 0.9)\n",
    "model(torch.rand(10,1))\n",
    "test_eq(10, sum([model[i].weight.abs().gt(0).sum() for i in (0,2)]))\n",
    "hooks.remove()\n",
    "for i in (0,2): model[i].weight.data = torch.ones_like(model[i].weight)\n",
    "model(torch.rand(10,1))\n",
    "test_eq(100, sum([model[i].weight.abs().gt(0).sum() for i in (0,2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop/Grow Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_score(p, **kwargs): return torch.rand_like(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_magnitude(p, **kwargs): return p.data.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_magnitude(p, **kwargs): return p.grad.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gradient_momentum(p, opt, **kwargs):\n",
    "    '''Calculates the momentum of the gradient for a parameter `p` from the `opt` state.'''\n",
    "    state = opt.state[p]\n",
    "    grad_avg = state['grad_avg'] if 'grad_avg' in state else None\n",
    "    sqr_avg = state['sqr_avg'] if 'sqr_avg' in state else None\n",
    "    if grad_avg is None:\n",
    "        raise Exception(f\"Error: 'grad_avg' key not found in optimizer state. Tip: set the `mom` hyperparamter in the learner.\")\n",
    "    if sqr_avg is None:\n",
    "        grad_mom = grad_avg\n",
    "    else:\n",
    "        try: eps = opt.state_dict()['hypers'][0]['eps']\n",
    "        except: eps = 1e-6\n",
    "        grad_mom =  grad_avg / (torch.sqrt(sqr_avg + eps))\n",
    "    return grad_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Sparse Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def top_k_mask(t, n_keep):\n",
    "    '''Returns a mask with `n_keep` ones cooresponding to the largest values in `t`'''\n",
    "    n_drop = t.numel() - n_keep\n",
    "    _, sorted_ixs = torch.topk(t.flatten(), k=t.numel())\n",
    "    mask = torch.cat([torch.ones(n_keep, dtype=torch.bool, device=t.device), \n",
    "                      torch.zeros(n_drop, dtype=torch.bool, device=t.device)])\n",
    "    mask = mask.scatter(0, sorted_ixs, mask)\n",
    "    return mask.view(*t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-0.9, 0.9, 20).reshape(4,5)\n",
    "mask = top_k_mask(t, 5)\n",
    "test_eq(0, mask[:3].sum())\n",
    "test_eq(5, mask[3:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DynamicSparseTrainingCallback(Callback):\n",
    "    '''Dynamically updates the network connectivity during training.'''\n",
    "    def __init__(self, sparse_modules=None,\n",
    "                 batches_per_update=None, initial_drop_grow_pct=0.3, stop_pct=0.75, \n",
    "                 keep_score_f=weight_magnitude, grow_score_f=gradient_magnitude):\n",
    "        store_attr('initial_drop_grow_pct,stop_pct,keep_score_f,grow_score_f,batches_per_update')\n",
    "        self.modules = sparse_modules\n",
    "        \n",
    "    def before_fit(self):\n",
    "        self.modules = ifnone(self.modules, sparseable_modules(self.learn.model))\n",
    "        self.batches_per_update = ifnone(self.batches_per_update, len(self.dls.train))\n",
    "        self.drop_grow_pct_sched = combine_scheds(\n",
    "            [self.stop_pct, 1-self.stop_pct],\n",
    "            [SchedCos(self.initial_drop_grow_pct, 0.), SchedNo(0.,0.)]\n",
    "        )\n",
    "    \n",
    "    def after_backward(self):\n",
    "        self.step()\n",
    "        if self.is_update_step:\n",
    "            for m in self.modules:\n",
    "                self.rewire_module(m)\n",
    "            raise CancelBatchException()\n",
    "        \n",
    "    def step(self):\n",
    "        if not self.training:\n",
    "            self.is_update_step = False\n",
    "        else:\n",
    "            step = self.epoch * self.n_iter + self.iter\n",
    "            n_steps = self.n_epoch * self.n_iter\n",
    "            pct_train = step / n_steps\n",
    "            self.drop_grow_pct = self.drop_grow_pct_sched(pct_train)\n",
    "            self.is_update_step = step > 0 and step % self.batches_per_update == 0 and self.drop_grow_pct > 0\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def rewire_module(self, m):\n",
    "        for param, param_name, sparsity in sparse_params(m):\n",
    "            if sparsity <= 0: continue\n",
    "\n",
    "            param, mask = m.weight, m.weight_mask\n",
    "\n",
    "            n_grow = int((1 - sparsity) * param.numel() * self.drop_grow_pct)\n",
    "            n_keep = int((1 - sparsity) * param.numel() * 1 - self.drop_grow_pct)\n",
    "\n",
    "            # determine which weights to keep\n",
    "            keep_score = self.keep_score_f(param, opt=self.learn.opt)\n",
    "            keep_mask = top_k_mask(keep_score, n_keep)\n",
    "\n",
    "            # determine which weights to grow\n",
    "            grow_score = self.grow_score_f(param, opt=self.learn.opt)\n",
    "            # make all keep weights to negative so we don't choose to grow them\n",
    "            grow_score = grow_score * keep_mask.logical_not() - keep_mask.float()\n",
    "            grow_mask = top_k_mask(grow_score, n_grow)\n",
    "\n",
    "            # update network connectivity\n",
    "            mask.data = keep_mask | grow_mask\n",
    "            \n",
    "            # zero momentum for new connections\n",
    "            self.reset_momentum(param, grow_mask & keep_mask.logical_not())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_momentum(self, p, mask):\n",
    "        state = self.opt.state[p]\n",
    "        if 'grad_avg' in state: state['grad_avg'].mul_(mask)\n",
    "        if 'sqr_avg' in state: state['sqr_avg'].mul_(mask)\n",
    "\n",
    "    _docs = dict(__init__='''Args:\n",
    "    module_sparsity_map: dictionary mapping modules to sparsity values\n",
    "    batches_per_update: # of batches per update, None (default) updates at end of each training epoch\n",
    "    initial_drop_grow_pct: percentage of weights to change during each dynamic weight update\n",
    "    stop_pct: stop dynamic weight updates after `stop_pct` of training\n",
    "    keep_score_f: function scoring each weight, top n are kept and the rest are zeroed\n",
    "    grow_score_f: function scoring each weight, top n excl. kept weights are unmasked and initialized to zero''',\n",
    "                 before_fit=\"Schedule the number of connections to drop & grow per update.\",\n",
    "                 before_batch=\"Add dynamic update hooks.\",\n",
    "                 after_backward=\"Remove dynamic update hooks and skip gradient update.\",\n",
    "                 step=\"Update self.is_update_step and self.drop_grow_pct.\",\n",
    "                 rewire_module=\"Update step for one module.\",\n",
    "                 reset_momentum=\"Initialize momentum to zero for newly-added connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"DynamicSparseTrainingCallback\" class=\"doc_header\"><code>class</code> <code>DynamicSparseTrainingCallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>DynamicSparseTrainingCallback</code>(**`sparse_modules`**=*`None`*, **`batches_per_update`**=*`None`*, **`initial_drop_grow_pct`**=*`0.3`*, **`stop_pct`**=*`0.75`*, **`keep_score_f`**=*`weight_magnitude`*, **`grow_score_f`**=*`gradient_magnitude`*) :: `Callback`\n",
       "\n",
       "Dynamically updates the network connectivity during training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DynamicSparseTrainingCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's test the callback on a toy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.367298</td>\n",
       "      <td>5.362422</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.590237</td>\n",
       "      <td>3.676831</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.637887</td>\n",
       "      <td>2.383177</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.929754</td>\n",
       "      <td>1.341745</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.343258</td>\n",
       "      <td>0.674901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.849479</td>\n",
       "      <td>0.260025</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.453780</td>\n",
       "      <td>0.182112</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.152167</td>\n",
       "      <td>0.085386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.918357</td>\n",
       "      <td>0.049471</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.733873</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.test_utils import *\n",
    "model = nn.Sequential(nn.Linear(1,32), nn.ReLU(), nn.Linear(32,32), nn.ReLU(), nn.Linear(32,1))\n",
    "learn = synth_learner(data=synth_dbunch(bs=100), model=model)\n",
    "sparse_hooks = sparsify_model(learn.model, 0.8, sparse_f=first_layer_dense_uniform)\n",
    "cbs = DynamicSparseTrainingCallback(batches_per_update=None, stop_pct=0.5, grow_score_f=gradient_momentum)\n",
    "learn.fit(10, lr=1e-2, cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test a slightly more realistic use case: MNIST_TINY on ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.779406</td>\n",
       "      <td>0.861908</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.952576</td>\n",
       "      <td>0.216129</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.668288</td>\n",
       "      <td>2.507498</td>\n",
       "      <td>0.746781</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.468921</td>\n",
       "      <td>0.129384</td>\n",
       "      <td>0.967096</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.348578</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.991416</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "from fastai.vision.all import *\n",
    "dls = ImageDataLoaders.from_folder(untar_data(URLs.MNIST_TINY))\n",
    "learn = cnn_learner(dls, resnet18, metrics=accuracy, pretrained=False)\n",
    "sparse_hooks = sparsify_model(learn.model, 0.9, erdos_renyi_sparsity)\n",
    "cbs = DynamicSparseTrainingCallback(batches_per_update=8, stop_pct=0.5, grow_score_f=gradient_momentum)\n",
    "learn.fit_one_cycle(5, 1e-2, cbs=cbs)\n",
    "\n",
    "test_close(1, learn.final_record[-1], eps=0.02) # better than 98% accuracy\n",
    "\n",
    "for m in sparseable_modules(learn.model):\n",
    "    for p, mask, s in sparse_params(m):\n",
    "        n_alive = p.abs().gt(0).sum()\n",
    "        n_total = p.numel()    \n",
    "        test_close(s, 1 - n_alive / n_total, eps=0.01) # layer sparsity = target sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preset Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Evolutionary Training (SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SET_presets = {'keep_score_f': weight_magnitude, 'grow_score_f': random_score, \n",
    "               'initial_drop_grow_pct': 0.3, 'stop_pct': 1.0,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Networks From Scratch (SNFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SNFS_presets = {'keep_score_f': weight_magnitude, 'grow_score_f': gradient_momentum, \n",
    "               'initial_drop_grow_pct': 0.3, 'stop_pct': 1.0,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rigged Lottery (RigL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "RigL_presets = {'keep_score_f': weight_magnitude, 'grow_score_f': gradient_magnitude, \n",
    "               'initial_drop_grow_pct':0.3, 'stop_pct':0.75,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
